{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Multivariate Timeseries / Transfer Learning Target Dataset\n",
    "\n",
    "##### with Average pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sktime\n",
    "from sktime.datasets import load_from_tsfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'Multivariate_ts/'\n",
    "\n",
    "dataset_lst = os.listdir(DATA_PATH)\n",
    "dataset_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, nb_classes, pre_model=None):\n",
    "\tinput_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "\tconv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n",
    "\tconv1 = keras.layers.normalization.batch_normalization.BatchNormalization()(conv1)\n",
    "\tconv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "\tconv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "\tconv2 = keras.layers.normalization.batch_normalization.BatchNormalization()(conv2)\n",
    "\tconv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "\tconv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "\tconv3 = keras.layers.normalization.batch_normalization.BatchNormalization()(conv3)\n",
    "\tconv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "\tgap_layer = keras.layers.pooling.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "\toutput_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "\tmodel = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\tif pre_model is not None:\n",
    "\n",
    "\t\tfor i in range(len(model.layers)-1):\n",
    "\t\t\tmodel.layers[i].set_weights(pre_model.layers[i].get_weights())\n",
    "\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(),\n",
    "\t\tmetrics=['accuracy'])\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logs(output_directory, hist, y_pred, y_true,duration,lr=True,y_true_val=None,y_pred_val=None):\n",
    "    hist_df = pd.DataFrame(hist.history)\n",
    "    hist_df.to_csv(output_directory+'history.csv', index=False)\n",
    "\n",
    "    df_metrics = calculate_metrics(y_true,y_pred, duration,y_true_val,y_pred_val)\n",
    "    df_metrics.to_csv(output_directory+'df_metrics.csv', index=False)\n",
    "\n",
    "    index_best_model = hist_df['loss'].idxmin() \n",
    "    row_best_model = hist_df.loc[index_best_model]\n",
    "\n",
    "    df_best_model = pd.DataFrame(data = np.zeros((1,6),dtype=np.float) , index = [0], \n",
    "        columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc', \n",
    "        'best_model_val_acc', 'best_model_learning_rate','best_model_nb_epoch'])\n",
    "    \n",
    "    print(row_best_model)\n",
    "    \n",
    "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
    "    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
    "    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n",
    "    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n",
    "    if lr == True:\n",
    "        # print('row_best_model')\n",
    "        # print(row_best_model)\n",
    "        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n",
    "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
    "\n",
    "    df_best_model.to_csv(output_directory+'df_best_model.csv', index=False)\n",
    "    # print('df_best_model')\n",
    "    # print(df_best_model)\n",
    "\n",
    "    # for FCN there is no hyperparameters fine tuning - everything is static in code \n",
    "\n",
    "    # plot losses \n",
    "    plot_epochs_metric(hist, output_directory+'epochs_loss.png')\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred,duration,y_true_val=None,y_pred_val=None): \n",
    "    res = pd.DataFrame(data = np.zeros((1,4),dtype=np.float), index=[0], \n",
    "        columns=['precision','accuracy','recall','duration'])\n",
    "    res['precision'] = precision_score(y_true,y_pred,average='macro')\n",
    "    res['accuracy'] = accuracy_score(y_true,y_pred)\n",
    "    \n",
    "    if not y_true_val is None:\n",
    "        # this is useful when transfer learning is used with cross validation\n",
    "        res['accuracy_val'] = accuracy_score(y_true_val,y_pred_val)\n",
    "\n",
    "    res['recall'] = recall_score(y_true,y_pred,average='macro')\n",
    "    res['duration'] = duration\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs_metric(hist, file_name, metric='loss'):\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history[metric])\n",
    "    plt.plot(hist.history['val_'+metric])\n",
    "    plt.title('model '+metric)\n",
    "    plt.ylabel(metric,fontsize='large')\n",
    "    plt.xlabel('epoch',fontsize='large')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(file_name,bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(y_train,y_test):\n",
    "    \"\"\"\n",
    "    Transform label to min equal zero and continuous \n",
    "    For example if we have [1,3,4] --->  [0,1,2]\n",
    "    \"\"\"\n",
    "    # init the encoder\n",
    "    encoder = LabelEncoder()\n",
    "    # concat train and test to fit\n",
    "    y_train_test = np.concatenate((y_train,y_test),axis =0)\n",
    "    # fit the encoder\n",
    "    encoder.fit(y_train_test)\n",
    "    # transform to min zero and continuous labels\n",
    "    new_y_train_test = encoder.transform(y_train_test)\n",
    "    # resplit the train and test\n",
    "    new_y_train = new_y_train_test[0:len(y_train)]\n",
    "    new_y_test = new_y_train_test[len(y_train):]\n",
    "    return new_y_train, new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train,y_train,x_test,y_test,callbacks,pre_model=None):\t\n",
    "\n",
    "    y_true_val = None\n",
    "    y_pred_val = None\n",
    "    \n",
    "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train,y_test),axis =0)))\n",
    "\n",
    "\t# make the min to zero of labels\n",
    "    y_train,y_test = transform_labels(y_train,y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.astype(np.int64)\n",
    "\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    if len(x_train.shape) == 2: # if univariate \n",
    "        # add a dimension to make it multivariate with one dimension \n",
    "        x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],1))\n",
    "        x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "    start_time = time.time()\n",
    "    # remove last layer to replace with a new one \n",
    "    input_shape = (None,x_train.shape[2])\n",
    "    model = build_model(input_shape, nb_classes,pre_model)\n",
    "\n",
    "    if verbose == True: \n",
    "        model.summary()\n",
    "\n",
    "    # b = model.layers[1].get_weights()\n",
    "\n",
    "    hist = model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
    "        verbose=verbose, validation_data=(x_test,y_test), callbacks=callbacks)\n",
    "\n",
    "    # a = model.layers[1].get_weights()\n",
    "\n",
    "    # compare_weights(a,b)\n",
    "\n",
    "    model = keras.models.load_model(file_path)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    # convert the predicted from binary to integer \n",
    "    y_pred = np.argmax(y_pred , axis=1)\n",
    "\n",
    "    duration = time.time()-start_time\n",
    "\n",
    "    df_metrics = save_logs(write_output_dir, hist, y_pred, y_true,\n",
    "                           duration,lr=True, y_true_val=y_true_val,\n",
    "                           y_pred_val=y_pred_val)\n",
    "\n",
    "    print('df_metrics')\n",
    "    print(df_metrics)\n",
    "\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory_path): \n",
    "    if os.path.exists(directory_path): \n",
    "        return None\n",
    "    else: \n",
    "        try: \n",
    "            os.makedirs(directory_path)\n",
    "        except: \n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            return None \n",
    "        return directory_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Multivariate Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_epochs = 2000\n",
    "verbose = False\n",
    "\n",
    "## \n",
    "for i in range(len(dataset_lst)):\n",
    "    data_num = i \n",
    "    data_name = dataset_lst[data_num]\n",
    "    train_x, train_y = load_from_tsfile(\n",
    "        os.path.join(DATA_PATH, \"{}/{}_TRAIN.ts\".format(data_name, data_name)), return_data_type=\"numpy3d\"\n",
    "    )\n",
    "    test_x, test_y = load_from_tsfile(\n",
    "        os.path.join(DATA_PATH, \"{}/{}_TEST.ts\".format(data_name, data_name)), return_data_type=\"numpy3d\"\n",
    "    )\n",
    "\n",
    "    train_X = np.transpose(train_x, (0,2,1))\n",
    "    test_X = np.transpose(test_x, (0,2,1))\n",
    "\n",
    "    results_dir = 'results/fcn/'\n",
    "    write_output_dir = results_dir + data_name + '/'\n",
    "    # set model output path\n",
    "    file_path = write_output_dir + 'best_model.hdf5'\n",
    "    # create directory\n",
    "    create_directory(write_output_dir)\n",
    "    # reduce learning rate\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                                                    patience=50,min_lr=0.0001)\n",
    "    # model checkpoint\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                        save_best_only=True)\n",
    "    callbacks=[reduce_lr,model_checkpoint]\n",
    "\n",
    "    train(train_X, train_y, test_X, test_y, callbacks,pre_model=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Target Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_epochs = 2000\n",
    "\n",
    "## Epoch 별 변화 출력여부\n",
    "verbose = True\n",
    "\n",
    "with open('../Data/data_no_std.pkl', 'rb') as f:\n",
    "     dataset = pickle.load(f)\n",
    "\n",
    "train_X = dataset[0]\n",
    "test_X = dataset[1]\n",
    "train_y = dataset[2]\n",
    "test_y = dataset[3]\n",
    "\n",
    "results_dir = 'results/'\n",
    "write_output_dir = results_dir + 'maneuver/'\n",
    "# set model output path\n",
    "file_path = write_output_dir + 'best_model.hdf5'\n",
    "# create directory\n",
    "create_directory(write_output_dir)\n",
    "# reduce learning rate\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                                                patience=50,min_lr=0.0001)\n",
    "# model checkpoint\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                    save_best_only=True)\n",
    "callbacks=[reduce_lr,model_checkpoint]\n",
    "\n",
    "train(train_X, train_y, test_X, test_y, callbacks,pre_model=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_lst)):\n",
    "    data_num = i \n",
    "    data_name = dataset_lst[data_num]\n",
    "    \n",
    "    with open('../Data/data_no_std.pkl', 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "\n",
    "    train_X = dataset[0]\n",
    "    test_X = dataset[1]\n",
    "    train_y = dataset[2]\n",
    "    test_y = dataset[3]\n",
    "\n",
    "    results_dir = 'results/transfer/'\n",
    "    write_output_dir = results_dir + data_name + '/'\n",
    "    # set model output path\n",
    "    file_path = write_output_dir + 'best_model.hdf5'\n",
    "    # create directory\n",
    "    create_directory(write_output_dir)\n",
    "    pre_model = keras.models.load_model('results/fcn/'+data_name+'/best_model.hdf5')\n",
    "    \n",
    "    # reduce learning rate\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                                                    patience=50,min_lr=0.0001)\n",
    "    # model checkpoint\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                        save_best_only=True)\n",
    "    callbacks=[reduce_lr,model_checkpoint]\n",
    "\n",
    "    train(train_X, train_y, test_X, test_y, callbacks,pre_model=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e34963b9aa1c0ebb4812a7f2a57632eb232e24c7240576deb31ccfc1dcd81fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
